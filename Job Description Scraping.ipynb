{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jessesw.com/Data-Science-Skills/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "# nltk.download() # download nltk dependent packages\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://www.indeed.com/viewjob?jk=5505e59f8e5a32a4&q=%22data+scientist%22&tk=19ftfgsmj19ti0l3&from=web&advn=1855944161169178&sjdu=QwrRXKrqZ3CNX5W-O9jEvWC1RT2wMYkGnZrqGdrncbKqQ7uwTLXzT1_ME9WQ4M-7om7mrHAlvyJT8cA_14IV5w&pub=pub-indeed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    try:\n",
    "        site = urllib.request.urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return   # Need this in case the website isn't there anymore or some other weird connection problem \n",
    "    \n",
    "    soup_obj = BeautifulSoup(site,'lxml') # Get the html from the site\n",
    "    \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "\n",
    "    text = soup_obj.get_text() # Get the text from this\n",
    "    \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "        \n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    \n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out  \n",
    "        \n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "        \n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "        \n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "    \n",
    "    text=text.decode('unicode_escape')\n",
    "       \n",
    "    text = re.sub('[^a-zA-Z.+3]',' ', text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                                # Also include + for C++\n",
    "        \n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "        \n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "        \n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts (we are just looking at whether a term existed\n",
    "                            # or not on the website)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['understands',\n",
       " 'please',\n",
       " 'measurement',\n",
       " 'indeedupload',\n",
       " 'zip',\n",
       " 'war',\n",
       " 'scientist',\n",
       " 'waboeing',\n",
       " 'per',\n",
       " 'termslet',\n",
       " 'benefits',\n",
       " 'resume.',\n",
       " 'si...let',\n",
       " 'sciences',\n",
       " 'frequency',\n",
       " 'coding',\n",
       " 'concisely',\n",
       " 'jobother',\n",
       " 'access',\n",
       " 'look']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = text_cleaner('http://www.indeed.com/viewjob?jk=5505e59f8e5a32a4&q=%22data+scientist%22&tk=19ftfgsmj19ti0l3&from=web&advn=1855944161169178&sjdu=QwrRXKrqZ3CNX5W-O9jEvWC1RT2wMYkGnZrqGdrncbKqQ7uwTLXzT1_ME9WQ4M-7om7mrHAlvyJT8cA_14IV5w&pub=pub-indeed')\n",
    "sample[:20] # Just show the first 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seattle_info = skills_info(city = 'Seattle', state = 'WA') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(seattle_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9b87470eed3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mskills_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Seattle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'WA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-07129373928f>\u001b[0m in \u001b[0;36mskills_info\u001b[0;34m(city, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtotal_num_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_numbers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_numbers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtotal_num_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_numbers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcity_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "skills_info(city = 'Seattle', state = 'WA') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There were', 623, 'jobs found,', 'Nationwide')\n",
      "('Getting page', 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8e6c64706d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskills_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-07129373928f>\u001b[0m in \u001b[0;36mskills_info\u001b[0;34m(city, state)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mjob_link_area\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'resultsCol'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The center column on the page where the job postings exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mjob_URLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_link_area\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Get the URLS for the jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mjob_URLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_URLS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'clk'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Now get just the job related URLS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-07129373928f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mjob_link_area\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'resultsCol'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The center column on the page where the job postings exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mjob_URLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_link_area\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Get the URLS for the jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mjob_URLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_URLS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'clk'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Now get just the job related URLS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not NoneType"
     ]
    }
   ],
   "source": [
    "all_info = skills_info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skills_info(city = None, state = None):\n",
    "    '''\n",
    "    This function will take a desired city/state and look for all new job postings\n",
    "    on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage for each skill\n",
    "    is then displayed at the end of the collation. \n",
    "        \n",
    "    Inputs: The location's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search (this can take a while!!!).\n",
    "    Input the city/state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "    \n",
    "    Output: A bar chart showing the most commonly desired skills in the job market for \n",
    "    a data scientist. \n",
    "    '''\n",
    "        \n",
    "    final_job = 'data+scientist' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "    \n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,\n",
    "                    '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "    base_url = 'http://www.indeed.com'\n",
    "    \n",
    "    try:\n",
    "        html = urllib.request.urlopen(final_site).read() # Open up the front page of our search first\n",
    "    except:\n",
    "        'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "        return\n",
    "    soup = BeautifulSoup(html) # Get the html from the first page\n",
    "    \n",
    "    # Now find out how many jobs there were\n",
    "    \n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                        # The 'searchCount' object has this\n",
    "        \n",
    "    num_jobs_area=num_jobs_area.decode('unicode_escape') # Need to put back from byte to string\n",
    "        \n",
    "    job_numbers = re.findall('\\d+', num_jobs_area) # Extract the total jobs found from the search result\n",
    "    \n",
    "    if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "        total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[2]) \n",
    "    \n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "        \n",
    "    print(('There were', total_num_jobs, 'jobs found,', city_title)) # Display how many jobs were found\n",
    "    \n",
    "    num_pages = total_num_jobs/10 # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "    \n",
    "    for i in range(1,int(num_pages)+1): # Loop through all of our search result pages\n",
    "        print(('Getting page', i))\n",
    "        start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "            \n",
    "        html_page = urllib.request.urlopen(current_page).read() # Get the page\n",
    "            \n",
    "        page_obj = BeautifulSoup(html_page,\"lxml\") # Locate all of the job links\n",
    "        job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "            \n",
    "        job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a')] # Get the URLS for the jobs\n",
    "            \n",
    "        job_URLS = [x for x in job_URLS if 'clk' in x] # Now get just the job related URLS\n",
    "            \n",
    "        for j in range(0,len(job_URLS)):\n",
    "            final_description = text_cleaner(job_URLS[j])\n",
    "            if final_description: # So that we only append when the website was accessed correctly\n",
    "                job_descriptions.append(final_description)\n",
    "            sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "        \n",
    "    print ('Done with collecting the job postings!')    \n",
    "    print(('There were', len(job_descriptions), 'jobs successfully found.'))\n",
    "    \n",
    "    doc_frequency = Counter() # This will create a full counter of our terms. \n",
    "    [doc_frequency.update(item) for item in job_descriptions] # List comp\n",
    "    \n",
    "    # Now we can just look at our final dict list inside doc_frequency\n",
    "    \n",
    "    # Obtain our key terms and store them in a dict. These are the key data science skills we are looking for\n",
    "    \n",
    "    prog_lang_dict = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\n",
    "                    'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\n",
    "                    'Ruby':doc_frequency['ruby'],\n",
    "                    'Perl':doc_frequency['perl'], 'Matlab':doc_frequency['matlab'],\n",
    "                    'JavaScript':doc_frequency['javascript'], 'Scala': doc_frequency['scala']})\n",
    "                      \n",
    "    analysis_tool_dict = Counter({'Excel':doc_frequency['excel'],  'Tableau':doc_frequency['tableau'],\n",
    "                        'D3.js':doc_frequency['d3.js'], 'SAS':doc_frequency['sas'],\n",
    "                        'SPSS':doc_frequency['spss'], 'D3':doc_frequency['d3']})  \n",
    "\n",
    "    hadoop_dict = Counter({'Hadoop':doc_frequency['hadoop'], 'MapReduce':doc_frequency['mapreduce'],\n",
    "                'Spark':doc_frequency['spark'], 'Pig':doc_frequency['pig'],\n",
    "                'Hive':doc_frequency['hive'], 'Shark':doc_frequency['shark'],\n",
    "                'Oozie':doc_frequency['oozie'], 'ZooKeeper':doc_frequency['zookeeper'],\n",
    "                'Flume':doc_frequency['flume'], 'Mahout':doc_frequency['mahout']})\n",
    "                \n",
    "    database_dict = Counter({'SQL':doc_frequency['sql'], 'NoSQL':doc_frequency['nosql'],\n",
    "                    'HBase':doc_frequency['hbase'], 'Cassandra':doc_frequency['cassandra'],\n",
    "                    'MongoDB':doc_frequency['mongodb']})\n",
    "                     \n",
    "               \n",
    "    overall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict # Combine our Counter objects\n",
    "    \n",
    "    \n",
    "    final_frame = pd.DataFrame(list(overall_total_skills.items()), columns = ['Term', 'NumPostings']) # Convert these terms to a \n",
    "                                                                                                # dataframe \n",
    "    \n",
    "    # Change the values to reflect a percentage of the postings \n",
    "    \n",
    "    final_frame.NumPostings = (final_frame.NumPostings)*100/len(job_descriptions) # Gives percentage of job postings \n",
    "                                                                                    #  having that term \n",
    "    \n",
    "    # Sort the data for plotting purposes\n",
    "    \n",
    "    final_frame.sort(columns = 'NumPostings', ascending = False, inplace = True)\n",
    "    \n",
    "    # Get it ready for a bar plot\n",
    "        \n",
    "    final_plot = final_frame.plot(x = 'Term', kind = 'bar', legend = None, \n",
    "                            title = 'Percentage of Data Scientist Job Ads with a Key Skill, ' + city_title)\n",
    "        \n",
    "    final_plot.set_ylabel('Percentage Appearing in Job Ads')\n",
    "    fig = final_plot.get_figure() # Have to convert the pandas plot object to a matplotlib object\n",
    "        \n",
    "        \n",
    "    return fig, final_frame # End of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_job = 'data+scientist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data+scientist'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = None\n",
    "state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There were', 621, 'jobs found,', 'Nationwide')\n"
     ]
    }
   ],
   "source": [
    "    final_job = 'data+scientist' # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "    \n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)\n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,\n",
    "                    '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "    base_url = 'http://www.indeed.com'\n",
    "    \n",
    "    try:\n",
    "        html = urllib.request.urlopen(final_site).read() # Open up the front page of our search first\n",
    "    except:\n",
    "        'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "    #    return\n",
    "    soup = BeautifulSoup(html) # Get the html from the first page\n",
    "    \n",
    "    # Now find out how many jobs there were\n",
    "    \n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                        # The 'searchCount' object has this\n",
    "        \n",
    "    num_jobs_area=num_jobs_area.decode('unicode_escape') # Need to put back from byte to string\n",
    "        \n",
    "    job_numbers = re.findall('\\d+', num_jobs_area) # Extract the total jobs found from the search result\n",
    "    \n",
    "    if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "        total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[2]) \n",
    "    \n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "        \n",
    "    print(('There were', total_num_jobs, 'jobs found,', city_title)) # Display how many jobs were found\n",
    "    \n",
    "    num_pages = total_num_jobs/10 # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'floor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-79fd2540664a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'floor' is not defined"
     ]
    }
   ],
   "source": [
    "floor(num_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Getting page', 1)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-230c1d584214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mjob_link_area\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'resultsCol'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The center column on the page where the job postings exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mjob_URLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_link_area\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Get the URLS for the jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mjob_URLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_URLS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'clk'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Now get just the job related URLS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-230c1d584214>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mjob_link_area\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'resultsCol'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The center column on the page where the job postings exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mjob_URLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_link_area\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Get the URLS for the jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mjob_URLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjob_URLS\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'clk'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Now get just the job related URLS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be str, not NoneType"
     ]
    }
   ],
   "source": [
    "    for i in range(1,int(num_pages)+1): # Loop through all of our search result pages\n",
    "        print(('Getting page', i))\n",
    "        start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "            \n",
    "        html_page = urllib.request.urlopen(current_page).read() # Get the page\n",
    "            \n",
    "        page_obj = BeautifulSoup(html_page,\"lxml\") # Locate all of the job links\n",
    "        job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "            \n",
    "        job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a')] # Get the URLS for the jobs\n",
    "            \n",
    "        job_URLS = [x for x in job_URLS if 'clk' in x] # Now get just the job related URLS\n",
    "            \n",
    "        for j in range(0,len(job_URLS)):\n",
    "            final_description = text_cleaner(job_URLS[j])\n",
    "            if final_description: # So that we only append when the website was accessed correctly\n",
    "                job_descriptions.append(final_description)\n",
    "            sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "        \n",
    "    print ('Done with collecting the job postings!')    \n",
    "    print(('There were', len(job_descriptions), 'jobs successfully found.'))\n",
    "    \n",
    "    doc_frequency = Counter() # This will create a full counter of our terms. \n",
    "    [doc_frequency.update(item) for item in job_descriptions] # List comp\n",
    "    \n",
    "    # Now we can just look at our final dict list inside doc_frequency\n",
    "    \n",
    "    # Obtain our key terms and store them in a dict. These are the key data science skills we are looking for\n",
    "    \n",
    "    prog_lang_dict = Counter({'R':doc_frequency['r'], 'Python':doc_frequency['python'],\n",
    "                    'Java':doc_frequency['java'], 'C++':doc_frequency['c++'],\n",
    "                    'Ruby':doc_frequency['ruby'],\n",
    "                    'Perl':doc_frequency['perl'], 'Matlab':doc_frequency['matlab'],\n",
    "                    'JavaScript':doc_frequency['javascript'], 'Scala': doc_frequency['scala']})\n",
    "                      \n",
    "    analysis_tool_dict = Counter({'Excel':doc_frequency['excel'],  'Tableau':doc_frequency['tableau'],\n",
    "                        'D3.js':doc_frequency['d3.js'], 'SAS':doc_frequency['sas'],\n",
    "                        'SPSS':doc_frequency['spss'], 'D3':doc_frequency['d3']})  \n",
    "\n",
    "    hadoop_dict = Counter({'Hadoop':doc_frequency['hadoop'], 'MapReduce':doc_frequency['mapreduce'],\n",
    "                'Spark':doc_frequency['spark'], 'Pig':doc_frequency['pig'],\n",
    "                'Hive':doc_frequency['hive'], 'Shark':doc_frequency['shark'],\n",
    "                'Oozie':doc_frequency['oozie'], 'ZooKeeper':doc_frequency['zookeeper'],\n",
    "                'Flume':doc_frequency['flume'], 'Mahout':doc_frequency['mahout']})\n",
    "                \n",
    "    database_dict = Counter({'SQL':doc_frequency['sql'], 'NoSQL':doc_frequency['nosql'],\n",
    "                    'HBase':doc_frequency['hbase'], 'Cassandra':doc_frequency['cassandra'],\n",
    "                    'MongoDB':doc_frequency['mongodb']})\n",
    "                     \n",
    "               \n",
    "    overall_total_skills = prog_lang_dict + analysis_tool_dict + hadoop_dict + database_dict # Combine our Counter objects\n",
    "    \n",
    "    \n",
    "    final_frame = pd.DataFrame(list(overall_total_skills.items()), columns = ['Term', 'NumPostings']) # Convert these terms to a \n",
    "                                                                                                # dataframe \n",
    "    \n",
    "    # Change the values to reflect a percentage of the postings \n",
    "    \n",
    "    final_frame.NumPostings = (final_frame.NumPostings)*100/len(job_descriptions) # Gives percentage of job postings \n",
    "                                                                                    #  having that term \n",
    "    \n",
    "    # Sort the data for plotting purposes\n",
    "    \n",
    "    final_frame.sort(columns = 'NumPostings', ascending = False, inplace = True)\n",
    "    \n",
    "    # Get it ready for a bar plot\n",
    "        \n",
    "    final_plot = final_frame.plot(x = 'Term', kind = 'bar', legend = None, \n",
    "                            title = 'Percentage of Data Scientist Job Ads with a Key Skill, ' + city_title)\n",
    "        \n",
    "    final_plot.set_ylabel('Percentage Appearing in Job Ads')\n",
    "    fig = final_plot.get_figure() # Have to convert the pandas plot object to a matplotlib object\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-cb03bc3419ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_frame\u001b[0m \u001b[0;31m# End of the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fig' is not defined"
     ]
    }
   ],
   "source": [
    "fig, final_frame # End of the function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
